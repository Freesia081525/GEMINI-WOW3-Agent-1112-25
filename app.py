"""
FDA Document Intelligence Workbench
Complete Streamlit application with OCR, word graphs, agent workflows, and advanced analytics
"""

import os
import io
import time
import base64
import json
import re
import hashlib
from datetime import datetime
from collections import Counter, defaultdict
from typing import List, Dict, Any, Optional
import numpy as np
import pandas as pd

# Core libraries
import streamlit as st
import yaml
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Image and PDF processing
from PIL import Image
import fitz  # PyMuPDF

# OCR libraries
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except:
    TESSERACT_AVAILABLE = False
    
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except:
    EASYOCR_AVAILABLE = False

# NLP libraries
try:
    import yake
    YAKE_AVAILABLE = True
except:
    YAKE_AVAILABLE = False

# LLM clients
import google.generativeai as genai
from openai import OpenAI
try:
    from xai_sdk import Client as XAIClient
    from xai_sdk.chat import user as grok_user, system as grok_system, image as grok_image
    GROK_AVAILABLE = True
except:
    GROK_AVAILABLE = False

# =============================================================================
# CONFIGURATION & CONSTANTS
# =============================================================================

LOCALIZATION = {
    "en": {
        "title": "üè• FDA Document Intelligence Workbench",
        "subtitle": "Advanced Document Analysis & Multi-Agent Processing System",
        "upload": "Upload Documents",
        "paste": "Paste Text Content",
        "add_paste": "Add Pasted Text",
        "docs": "üìÑ Documents",
        "ocr": "üîç OCR Processing",
        "combine": "üìä Combine & Analyze",
        "agents": "ü§ñ Agent Workflows",
        "dashboard": "üìà Analytics Dashboard",
        "wordgraph": "üìä Word Graph Analysis",
        "settings": "‚öôÔ∏è Settings",
        "api_keys": "üîë API Keys",
        "theme": "Theme",
        "language": "Language",
        "style": "Visual Style",
        "upload_hint": "Support: PDF, TXT, MD, CSV, JSON",
        "ocr_mode": "OCR Mode",
        "ocr_python": "Python OCR",
        "ocr_llm": "LLM OCR",
        "ocr_lang": "OCR Language",
        "run_ocr": "Run OCR",
        "preview": "Preview",
        "edit": "Edit",
        "delete": "Delete",
        "page": "Page",
        "keywords": "Keywords",
        "auto_extract": "Auto Extract",
        "generate_combined": "Generate Combined Document",
        "combined_doc": "Combined Document",
        "select_agents": "Select Agents to Run",
        "run_agent": "Execute Agent",
        "agent_output": "Agent Output",
        "metrics": "Metrics",
        "export": "Export",
        "word_freq": "Word Frequency",
        "word_cloud": "Word Cloud",
        "ngram_analysis": "N-gram Analysis",
        "entity_extraction": "Entity Extraction",
        "sentiment": "Sentiment Analysis",
        "compliance_check": "Compliance Check",
        "risk_analysis": "Risk Analysis",
        "timeline": "Timeline Analysis",
        "docs_processed": "Documents Processed",
        "pages_ocr": "Pages OCR'd",
        "tokens": "Total Tokens",
        "agent_runs": "Agent Runs",
        "processing_time": "Processing Time",
        "success": "Success",
        "error": "Error",
        "warning": "Warning",
        "info": "Info",
        "gemini_key": "Gemini API Key",
        "openai_key": "OpenAI API Key",
        "grok_key": "Grok API Key",
        "apply_keys": "Apply Keys",
        "saved": "Saved successfully",
        "failed": "Operation failed",
        "loading": "Loading...",
        "batch_size": "Batch Size",
        "temperature": "Temperature",
        "max_tokens": "Max Tokens",
        "top_words": "Top Words",
        "bigrams": "Bigrams",
        "trigrams": "Trigrams",
        "co_occurrence": "Co-occurrence Network",
        "fda_features": "üî¨ FDA-Specific Features",
        "adverse_events": "Adverse Event Detection",
        "drug_interactions": "Drug Interaction Analysis",
        "regulatory_compliance": "Regulatory Compliance Check"
    },
    "zh-TW": {
        "title": "üè• FDA Êñá‰ª∂Êô∫ËÉΩÂ∑•‰ΩúÂè∞",
        "subtitle": "ÈÄ≤ÈöéÊñá‰ª∂ÂàÜÊûêËàáÂ§ö‰ª£ÁêÜËôïÁêÜÁ≥ªÁµ±",
        "upload": "‰∏äÂÇ≥Êñá‰ª∂",
        "paste": "Ë≤º‰∏äÊñáÂ≠óÂÖßÂÆπ",
        "add_paste": "Êñ∞Â¢ûË≤º‰∏äÊñáÂ≠ó",
        "docs": "üìÑ Êñá‰ª∂",
        "ocr": "üîç OCR ËôïÁêÜ",
        "combine": "üìä Âêà‰ΩµËàáÂàÜÊûê",
        "agents": "ü§ñ ‰ª£ÁêÜÂ∑•‰ΩúÊµÅÁ®ã",
        "dashboard": "üìà ÂàÜÊûêÂÑÄË°®Êùø",
        "wordgraph": "üìä Ë©ûÂΩôÂúñÂàÜÊûê",
        "settings": "‚öôÔ∏è Ë®≠ÂÆö",
        "api_keys": "üîë API ÈáëÈë∞",
        "theme": "‰∏ªÈ°å",
        "language": "Ë™ûË®Ä",
        "style": "Ë¶ñË¶∫È¢®Ê†º",
        "upload_hint": "ÊîØÊè¥Ê†ºÂºèÔºöPDF„ÄÅTXT„ÄÅMD„ÄÅCSV„ÄÅJSON",
        "ocr_mode": "OCR Ê®°Âºè",
        "ocr_python": "Python OCR",
        "ocr_llm": "LLM OCR",
        "ocr_lang": "OCR Ë™ûË®Ä",
        "run_ocr": "Âü∑Ë°å OCR",
        "preview": "È†êË¶Ω",
        "edit": "Á∑®ËºØ",
        "delete": "Âà™Èô§",
        "page": "È†Å",
        "keywords": "ÈóúÈçµÂ≠ó",
        "auto_extract": "Ëá™ÂãïÊì∑Âèñ",
        "generate_combined": "ÁîüÊàêÂêà‰ΩµÊñá‰ª∂",
        "combined_doc": "Âêà‰ΩµÊñá‰ª∂",
        "select_agents": "ÈÅ∏ÊìáË¶ÅÂü∑Ë°åÁöÑ‰ª£ÁêÜ",
        "run_agent": "Âü∑Ë°å‰ª£ÁêÜ",
        "agent_output": "‰ª£ÁêÜËº∏Âá∫",
        "metrics": "ÊåáÊ®ô",
        "export": "ÂåØÂá∫",
        "word_freq": "Ë©ûÈ†ª",
        "word_cloud": "Ë©ûÈõ≤",
        "ngram_analysis": "N-gram ÂàÜÊûê",
        "entity_extraction": "ÂØ¶È´îÊì∑Âèñ",
        "sentiment": "ÊÉÖÊÑüÂàÜÊûê",
        "compliance_check": "ÂêàË¶èÊ™¢Êü•",
        "risk_analysis": "È¢®Èö™ÂàÜÊûê",
        "timeline": "ÊôÇÈñìËª∏ÂàÜÊûê",
        "docs_processed": "Â∑≤ËôïÁêÜÊñá‰ª∂",
        "pages_ocr": "Â∑≤ OCR È†ÅÊï∏",
        "tokens": "Á∏Ω‰ª£Âπ£Êï∏",
        "agent_runs": "‰ª£ÁêÜÂü∑Ë°åÊ¨°Êï∏",
        "processing_time": "ËôïÁêÜÊôÇÈñì",
        "success": "ÊàêÂäü",
        "error": "ÈåØË™§",
        "warning": "Ë≠¶Âëä",
        "info": "Ë≥áË®ä",
        "gemini_key": "Gemini API ÈáëÈë∞",
        "openai_key": "OpenAI API ÈáëÈë∞",
        "grok_key": "Grok API ÈáëÈë∞",
        "apply_keys": "Â•óÁî®ÈáëÈë∞",
        "saved": "ÂÑ≤Â≠òÊàêÂäü",
        "failed": "Êìç‰ΩúÂ§±Êïó",
        "loading": "ËºâÂÖ•‰∏≠...",
        "batch_size": "ÊâπÊ¨°Â§ßÂ∞è",
        "temperature": "Ê∫´Â∫¶",
        "max_tokens": "ÊúÄÂ§ß‰ª£Âπ£Êï∏",
        "top_words": "ÁÜ±ÈñÄË©ûÂΩô",
        "bigrams": "ÈõôË©ûÁµÑ",
        "trigrams": "‰∏âË©ûÁµÑ",
        "co_occurrence": "ÂÖ±ÁèæÁ∂≤Áµ°",
        "fda_features": "üî¨ FDA Â∞àÁî®ÂäüËÉΩ",
        "adverse_events": "‰∏çËâØ‰∫ã‰ª∂ÂÅµÊ∏¨",
        "drug_interactions": "Ëó•Áâ©‰∫§‰∫í‰ΩúÁî®ÂàÜÊûê",
        "regulatory_compliance": "Ê≥ïË¶èÂêàË¶èÊ™¢Êü•"
    }
}

FLOWER_THEMES = [
    ("Áé´Áë∞Áü≥Ëã± Rose Quartz", "#e91e63", "#ffe4ec", "#1a1a1a", "#ffffff"),
    ("Ëñ∞Ë°£ËçâÈúß Lavender Mist", "#9c27b0", "#f3e5f5", "#1a1a1a", "#ffffff"),
    ("ÂêëÊó•ËëµÂÖâ Sunflower Glow", "#fbc02d", "#fff8e1", "#1a1a1a", "#ffffff"),
    ("Ê´ªËä± Cherry Blossom", "#ec407a", "#fde2ea", "#1a1a1a", "#ffffff"),
    ("Ëò≠Ëä±Á∂ªÊîæ Orchid Bloom", "#ab47bc", "#f4e1f7", "#1a1a1a", "#ffffff"),
    ("Áâ°‰∏πÁ≤â Peony Pink", "#f06292", "#fde1ee", "#1a1a1a", "#ffffff"),
    ("È≥∂Â∞æËóç Iris Indigo", "#3f51b5", "#e8eaf6", "#1a1a1a", "#ffffff"),
    ("Ëê¨Â£ΩËèä Marigold", "#ffa000", "#fff3e0", "#1a1a1a", "#ffffff"),
    ("ËìÆËä± Lotus", "#8e24aa", "#f5e1ff", "#1a1a1a", "#ffffff"),
    ("Ëå∂Ëä± Camellia", "#d81b60", "#fde1ea", "#1a1a1a", "#ffffff"),
    ("ËåâËéâ Jasmine", "#43a047", "#e8f5e9", "#1a1a1a", "#ffffff"),
    ("È¨±ÈáëÈ¶ôÁ¥Ö Tulip Red", "#e53935", "#ffebee", "#1a1a1a", "#ffffff"),
    ("Â§ßÈ∫óËä±Á¥´ Dahlia Plum", "#6a1b9a", "#ede7f6", "#1a1a1a", "#ffffff"),
    ("Ê¢îÂ≠êËä± Gardenia", "#009688", "#e0f2f1", "#1a1a1a", "#ffffff"),
    ("Áπ°ÁêÉËä± Hydrangea", "#5c6bc0", "#e3e8fd", "#1a1a1a", "#ffffff"),
    ("Èå¶Ëëµ Lavatera", "#7b1fa2", "#f2e5ff", "#1a1a1a", "#ffffff"),
    ("Ê´ªËçâ Primrose", "#f57c00", "#fff3e0", "#1a1a1a", "#ffffff"),
    ("È¢®Èà¥Ëçâ Bluebell", "#1e88e5", "#e3f2fd", "#1a1a1a", "#ffffff"),
    ("Êú®Ëò≠ Magnolia", "#8d6e63", "#efebe9", "#1a1a1a", "#ffffff"),
    ("Á¥´Ëó§ Wisteria", "#7e57c2", "#ede7f6", "#1a1a1a", "#ffffff"),
]

ADVANCED_PROMPTS = {
    "ocr": """‰Ω†ÊòØ‰∏Ä‰ΩçÁ≤æÁ¢∫ÁöÑ OCR ËΩâÈåÑÂ∞àÂÆ∂„ÄÇË´ãÈÄêÂ≠óËΩâÈåÑÊñáÊú¨ÔºåÂåÖÊã¨Ê®ôÈªûÁ¨¶ËôüÂíåÊèõË°å„ÄÇ

Ë¶ÅÊ±ÇÔºö
- ÁõÆÊ®ôË™ûË®ÄÔºö{language}
- ‰øùÁïôË°®Ê†ºÂíåÁ®ãÂºèÁ¢ºÂçÄÂ°äÔºà‰ΩøÁî® Markdown Ë°®Ê†º / ``` ÂçÄÂ°äÔºâ
- ‰∏çË¶ÅÊèèËø∞ÂúñÁâáÔºåÂÉÖËøîÂõûËΩâÈåÑÁöÑÊñáÊú¨
- ‰øùÊåÅÂéüÂßãÊ†ºÂºèÂíåÁµêÊßã
""",
    "agent_system": """‰Ω†ÊòØ‰∏ÄÂÄãÂèØÈù†„ÄÅÂÆâÂÖ®‰∏îÈ´òÊïàÁöÑÂ∞àÂÆ∂‰ª£ÁêÜ„ÄÇÁõÆÊ®ôÔºö
- Âö¥Ê†ºÈÅµÂæ™Á≥ªÁµ±Âíå‰ΩøÁî®ËÄÖÊåá‰ª§
- ÈªòÈªòÊé®ÁêÜÔºõÂÉÖËøîÂõûÊúÄÁµÇÁ≠îÊ°àÔºàÁÑ°ÊÄùËÄÉÈèàÔºâ
- Á∞°ÊΩî„ÄÅÁµêÊßãÂåñ„ÄÅÂø†ÂØ¶ÊñºËº∏ÂÖ•
- ÈÅøÂÖçÂπªË¶∫ÔºõÂ¶ÇÊûúË≠âÊìöÁº∫Â§±ÔºåË´ãË™™„ÄåÊú™Áü•„Äç
"""
}

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def estimate_tokens(text: str) -> int:
    """Estimate token count (rough heuristic)"""
    return max(1, len(text) // 4)

def hash_content(content: str) -> str:
    """Generate hash for content deduplication"""
    return hashlib.md5(content.encode()).hexdigest()[:12]

def extract_text_from_file(file) -> str:
    """Extract text from uploaded file"""
    suffix = file.name.lower().split(".")[-1]
    content = file.read()
    
    if suffix in ["txt", "md", "markdown"]:
        return content.decode("utf-8", errors="ignore")
    elif suffix == "csv":
        df = pd.read_csv(io.BytesIO(content))
        return df.to_markdown(index=False)
    elif suffix == "json":
        try:
            obj = json.loads(content.decode("utf-8", errors="ignore"))
            return "```json\n" + json.dumps(obj, indent=2, ensure_ascii=False) + "\n```"
        except:
            return content.decode("utf-8", errors="ignore")
    elif suffix == "pdf":
        return ""  # Handled separately
    else:
        return content.decode("utf-8", errors="ignore")

def pdf_to_images(pdf_bytes: bytes, dpi: int = 150) -> List[Dict]:
    """Convert PDF to images"""
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    images = []
    for i, page in enumerate(doc):
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append({"page": i+1, "image": img})
    doc.close()
    return images

def img_to_bytes(img: Image.Image) -> bytes:
    """Convert PIL Image to bytes"""
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()

def python_ocr(image: Image.Image, engine: str = "easyocr", language: str = "en") -> str:
    """Perform Python-based OCR"""
    if engine == "tesseract" and TESSERACT_AVAILABLE:
        lang_map = {"en": "eng", "zh": "chi_tra"}
        try:
            return pytesseract.image_to_string(image, lang=lang_map.get(language, "eng"))
        except:
            pass
    
    if EASYOCR_AVAILABLE:
        lang_map = {"en": "en", "zh": "ch_tra"}
        reader = easyocr.Reader([lang_map.get(language, "en")], gpu=False)
        result = reader.readtext(np.array(image), detail=0, paragraph=True)
        return "\n".join(result)
    
    return "OCR libraries not available"

def extract_keywords_yake(text: str, max_k: int = 20, language: str = "en") -> List[str]:
    """Extract keywords using YAKE"""
    if not YAKE_AVAILABLE:
        return []
    
    lang_map = {"en": "en", "zh": "zh"}
    kw_extractor = yake.KeywordExtractor(lan=lang_map.get(language, "en"), n=1, top=max_k)
    keywords = [k for k, s in kw_extractor.extract_keywords(text)]
    return keywords

def highlight_keywords(text: str, keywords: List[str], color: str = "coral") -> str:
    """Highlight keywords in text"""
    if not keywords:
        return text
    
    for kw in sorted(set(keywords), key=len, reverse=True):
        if kw:
            pattern = re.compile(rf"\b({re.escape(kw)})\b", re.IGNORECASE)
            text = pattern.sub(
                lambda m: f"<span style='color: {color}; font-weight: 600; background: {color}20; padding: 2px 4px; border-radius: 3px'>{m.group(0)}</span>",
                text
            )
    return text

def create_word_frequency(text: str, top_n: int = 50) -> pd.DataFrame:
    """Create word frequency dataframe"""
    # Simple tokenization
    words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]{2,}\b', text.lower())
    
    # Remove common stop words
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
                  'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
                  'this', 'that', 'these', 'those', 'ÁöÑ', '‰∫Ü', 'Âú®', 'ÊòØ', 'Êàë', 'Êúâ', 'Âíå'}
    words = [w for w in words if w not in stop_words and len(w) > 2]
    
    word_counts = Counter(words)
    df = pd.DataFrame(word_counts.most_common(top_n), columns=['Word', 'Frequency'])
    return df

def create_ngrams(text: str, n: int = 2, top_k: int = 20) -> List[tuple]:
    """Create n-grams from text"""
    words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]{2,}\b', text.lower())
    ngrams = zip(*[words[i:] for i in range(n)])
    ngram_counts = Counter([' '.join(ng) for ng in ngrams])
    return ngram_counts.most_common(top_k)

def create_cooccurrence_matrix(text: str, keywords: List[str], window: int = 5) -> pd.DataFrame:
    """Create word co-occurrence matrix"""
    words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]{2,}\b', text.lower())
    keywords_lower = [k.lower() for k in keywords]
    
    cooccur = defaultdict(lambda: defaultdict(int))
    
    for i, word in enumerate(words):
        if word in keywords_lower:
            for j in range(max(0, i-window), min(len(words), i+window+1)):
                if i != j and words[j] in keywords_lower:
                    cooccur[word][words[j]] += 1
    
    # Convert to dataframe
    df = pd.DataFrame(cooccur).fillna(0)
    return df

# =============================================================================
# LLM CLIENT WRAPPER
# =============================================================================

class LLMRouter:
    """Unified LLM client for multiple providers"""
    
    def __init__(self, google_key=None, openai_key=None, grok_key=None):
        self.google_key = google_key or os.getenv("GOOGLE_API_KEY")
        self.openai_key = openai_key or os.getenv("OPENAI_API_KEY")
        self.grok_key = grok_key or os.getenv("XAI_API_KEY")
        
        self._gemini = None
        self._openai = None
        self._grok = None
    
    def _init_gemini(self):
        if self._gemini is None and self.google_key:
            genai.configure(api_key=self.google_key)
            self._gemini = genai
        return self._gemini
    
    def _init_openai(self):
        if self._openai is None and self.openai_key:
            self._openai = OpenAI(api_key=self.openai_key)
        return self._openai
    
    def _init_grok(self):
        if self._grok is None and self.grok_key and GROK_AVAILABLE:
            self._grok = XAIClient(api_key=self.grok_key, timeout=3600)
        return self._grok
    
    def generate_text(self, provider: str, model: str, system_prompt: str, 
                     user_prompt: str, temperature: float = 0.2, 
                     max_tokens: int = 1500) -> str:
        """Generate text completion"""
        provider = provider.lower()
        
        try:
            if provider == "gemini":
                gem = self._init_gemini()
                if not gem:
                    raise ValueError("Gemini not configured")
                
                m = gem.GenerativeModel(model)
                parts = []
                if system_prompt:
                    parts.append({"role": "user", "parts": [f"System: {system_prompt}"]})
                parts.append({"role": "user", "parts": [user_prompt]})
                
                resp = m.generate_content(parts, generation_config={
                    "temperature": temperature,
                    "max_output_tokens": max_tokens
                })
                return resp.text or ""
            
            elif provider == "openai":
                oai = self._init_openai()
                if not oai:
                    raise ValueError("OpenAI not configured")
                
                resp = oai.chat.completions.create(
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    messages=[
                        {"role": "system", "content": system_prompt or ""},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                return resp.choices[0].message.content
            
            elif provider == "grok":
                client = self._init_grok()
                if not client:
                    raise ValueError("Grok not configured")
                
                chat = client.chat.create(model=model)
                if system_prompt:
                    chat.append(grok_system(system_prompt))
                chat.append(grok_user(user_prompt))
                response = chat.sample()
                return response.content
            
            else:
                raise ValueError(f"Unknown provider: {provider}")
        
        except Exception as e:
            return f"Error: {str(e)}"
    
    def ocr_image(self, provider: str, model: str, image_bytes: bytes,
                  prompt: str, temperature: float = 0.1, max_tokens: int = 2000) -> str:
        """Perform LLM-based OCR"""
        provider = provider.lower()
        
        try:
            if provider == "gemini":
                gem = self._init_gemini()
                if not gem:
                    raise ValueError("Gemini not configured")
                
                m = gem.GenerativeModel(model)
                b64 = base64.b64encode(image_bytes).decode("utf-8")
                img_part = {"inline_data": {"mime_type": "image/png", "data": b64}}
                
                resp = m.generate_content([prompt, img_part], generation_config={
                    "temperature": temperature,
                    "max_output_tokens": max_tokens
                })
                return resp.text or ""
            
            elif provider == "openai":
                oai = self._init_openai()
                if not oai:
                    raise ValueError("OpenAI not configured")
                
                b64 = base64.b64encode(image_bytes).decode("utf-8")
                resp = oai.chat.completions.create(
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    messages=[
                        {"role": "system", "content": "You are a meticulous OCR transcriber."},
                        {"role": "user", "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64}"}}
                        ]}
                    ]
                )
                return resp.choices[0].message.content
            
            else:
                return "Provider not supported for OCR"
        
        except Exception as e:
            return f"OCR Error: {str(e)}"

# =============================================================================
# UI STYLING
# =============================================================================

def apply_theme(theme_idx: int, dark_mode: bool):
    """Apply visual theme"""
    name, primary, bg_light, text_dark, text_light = FLOWER_THEMES[theme_idx]
    
    bg_color = "#1a1a1a" if dark_mode else bg_light
    text_color = text_light if dark_mode else text_dark
    card_bg = "#2d2d2d" if dark_mode else "#ffffff"
    border_color = f"{primary}40"
    
    st.markdown(f"""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
        
        :root {{
            --primary: {primary};
            --bg: {bg_color};
            --text: {text_color};
            --card-bg: {card_bg};
            --border: {border_color};
        }}
        
        .stApp {{
            background: linear-gradient(135deg, {bg_color} 0%, {primary}15 100%);
            font-family: 'Inter', sans-serif;
            color: var(--text);
        }}
        
        .main-header {{
            background: linear-gradient(90deg, {primary} 0%, {primary}cc 100%);
            padding: 2rem;
            border-radius: 16px;
            margin-bottom: 2rem;
            box-shadow: 0 8px 32px {primary}30;
            color: white;
            text-align: center;
        }}
        
        .main-title {{
            font-size: 2.5rem;
            font-weight: 700;
            margin: 0;
            text-shadow: 0 2px 10px rgba(0,0,0,0.2);
        }}
        
        .main-subtitle {{
            font-size: 1.1rem;
            font-weight: 300;
            margin-top: 0.5rem;
            opacity: 0.95;
        }}
        
        .card {{
            background: var(--card-bg);
            border: 2px solid var(--border);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1rem 0;
            box-shadow: 0 4px 16px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
        }}
        
        .card:hover {{
            transform: translateY(-2px);
            box-shadow: 0 8px 24px rgba(0,0,0,0.15);
            border-color: {primary};
        }}
        
        .metric-card {{
            background: linear-gradient(135deg, var(--card-bg) 0%, {primary}10 100%);
            border: 2px solid var(--border);
            border-radius: 12px;
            padding: 1.5rem;
            text-align: center;
            transition: all 0.3s ease;
        }}
        
        .metric-card:hover {{
            transform: scale(1.05);
            border-color: {primary};
        }}
        
        .metric-value {{
            font-size: 2.5rem;
            font-weight: 700;
            color: {primary};
            margin: 0.5rem 0;
        }}
        
        .metric-label {{
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.7;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }}
        
        .tag {{
            display: inline-block;
            padding: 0.4rem 0.8rem;
            margin: 0.25rem;
            border-radius: 20px;
            background: {primary}20;
            color: {primary};
            font-weight: 600;
            font-size: 0.85rem;
            border: 1px solid {primary}50;
            transition: all 0.2s ease;
        }}
        
        .tag:hover {{
            background: {primary}30;
            transform: scale(1.05);
        }}
        
        .status-indicator {{
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            font-weight: 600;
            font-size: 0.9rem;
        }}
        
        .status-success {{
            background: #4caf5020;
            color: #4caf50;
            border: 1px solid #4caf5050;
        }}
        
        .status-warning {{
            background: #ff980020;
            color: #ff9800;
            border: 1px solid #ff980050;
        }}
        
        .status-error {{
            background: #f4433620;
            color: #f44336;
            border: 1px solid #f4433650;
        }}
        
        .status-dot {{
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: currentColor;
            animation: pulse 2s infinite;
        }}
        
        @keyframes pulse {{
            0%, 100% {{ opacity: 1; transform: scale(1); }}
            50% {{ opacity: 0.5; transform: scale(0.8); }}
        }}
        
        .stButton > button {{
            background: linear-gradient(90deg, {primary} 0%, {primary}dd 100%);
            color: white;
            border: none;
            border-radius: 8px;
            padding: 0.6rem 1.5rem;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px {primary}30;
        }}
        
        .stButton > button:hover {{
            transform: translateY(-2px);
            box-shadow: 0 6px 20px {primary}40;
        }}
        
        .stTextInput > div > div > input:focus,
        .stTextArea > div > div > textarea:focus {{
            border-color: {primary};
            box-shadow: 0 0 0 3px {primary}20;
        }}
        
        .stSelectbox > div > div {{
            background: var(--card-bg);
            border: 2px solid var(--border);
            border-radius: 8px;
        }}
        
        .plot-container {{
            background: var(--card-bg);
            border-radius: 12px;
            padding: 1rem;
            box-shadow: 0 4px 16px rgba(0,0,0,0.1);
        }}
        
        .agent-workflow {{
            position: relative;
            padding-left: 2rem;
            border-left: 3px solid {primary}40;
            margin: 1rem 0;
        }}
        
        .agent-step {{
            position: relative;
            margin: 1.5rem 0;
        }}
        
        .agent-step::before {{
            content: '';
            position: absolute;
            left: -2.5rem;
            top: 50%;
            transform: translateY(-50%);
            width: 1rem;
            height: 1rem;
            border-radius: 50%;
            background: {primary};
            border: 3px solid var(--bg);
            box-shadow: 0 0 0 3px {primary}40;
        }}
        
        .expander {{
            background: var(--card-bg);
            border: 2px solid var(--border);
            border-radius: 12px;
            margin: 0.5rem 0;
        }}
        
        div[data-testid="stExpander"] {{
            background: var(--card-bg);
            border: 2px solid var(--border);
            border-radius: 12px;
        }}
        
        .sidebar .sidebar-content {{
            background: var(--card-bg);
        }}
        
        h1, h2, h3, h4, h5, h6 {{
            color: var(--text);
            font-weight: 600;
        }}
        
        .stTabs [data-baseweb="tab-list"] {{
            gap: 8px;
        }}
        
        .stTabs [data-baseweb="tab"] {{
            background: var(--card-bg);
            border: 2px solid var(--border);
            border-radius: 8px 8px 0 0;
            padding: 0.75rem 1.5rem;
            font-weight: 600;
            transition: all 0.3s ease;
        }}
        
        .stTabs [data-baseweb="tab"]:hover {{
            background: {primary}10;
            border-color: {primary};
        }}
        
        .stTabs [aria-selected="true"] {{
            background: {primary};
            color: white;
            border-color: {primary};
        }}
        </style>
    """, unsafe_allow_html=True)

def render_header(T: dict, theme_name: str):
    """Render main header"""
    st.markdown(f"""
        <div class="main-header">
            <div class="main-title">{T['title']}</div>
            <div class="main-subtitle">{T['subtitle']}</div>
            <div style="margin-top: 1rem;">
                <span class="tag">{theme_name}</span>
                <span class="tag">v2.0</span>
            </div>
        </div>
    """, unsafe_allow_html=True)

def render_metric_card(label: str, value: any, icon: str = "üìä"):
    """Render metric card"""
    st.markdown(f"""
        <div class="metric-card">
            <div style="font-size: 2rem;">{icon}</div>
            <div class="metric-value">{value}</div>
            <div class="metric-label">{label}</div>
        </div>
    """, unsafe_allow_html=True)

def render_status(status: str, message: str):
    """Render status indicator"""
    status_map = {
        "success": ("‚úì", "status-success"),
        "warning": ("‚ö†", "status-warning"),
        "error": ("‚úó", "status-error"),
        "info": ("‚Ñπ", "status-success")
    }
    icon, css_class = status_map.get(status, ("‚Ä¢", "status-success"))
    
    st.markdown(f"""
        <div class="status-indicator {css_class}">
            <span class="status-dot"></span>
            <span>{icon} {message}</span>
        </div>
    """, unsafe_allow_html=True)

# =============================================================================
# SESSION STATE INITIALIZATION
# =============================================================================

def init_session_state():
    """Initialize session state variables"""
    defaults = {
        "docs": [],
        "ocr_results": {},
        "combined_doc": "",
        "keywords": [],
        "agents": [],
        "agents_yaml": "",
        "agent_results": [],
        "metrics": {
            "docs_processed": 0,
            "pages_ocr": 0,
            "total_tokens": 0,
            "agent_runs": 0,
            "processing_times": []
        },
        "api_keys": {
            "gemini": None,
            "openai": None,
            "grok": None
        },
        "settings": {
            "lang": "zh-TW",
            "theme_idx": 0,
            "dark_mode": True,
            "ocr_engine": "easyocr",
            "ocr_language": "zh",
            "default_temperature": 0.2,
            "default_max_tokens": 1500
        },
        "word_analysis": {
            "word_freq": None,
            "bigrams": None,
            "trigrams": None,
            "cooccurrence": None
        }
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# =============================================================================
# MAIN APPLICATION
# =============================================================================

def main():
    st.set_page_config(
        page_title="FDA Document Intelligence",
        page_icon="üè•",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    init_session_state()
    
    # Sidebar settings
    with st.sidebar:
        st.title("‚öôÔ∏è " + LOCALIZATION[st.session_state.settings["lang"]]["settings"])
        
        # Language selection
        lang = st.selectbox(
            "üåê Language / Ë™ûË®Ä",
            ["en", "zh-TW"],
            index=0 if st.session_state.settings["lang"] == "en" else 1,
            key="lang_select"
        )
        st.session_state.settings["lang"] = lang
        T = LOCALIZATION[lang]
        
        # Theme selection
        st.subheader(T["theme"])
        theme_idx = st.selectbox(
            T["style"],
            range(len(FLOWER_THEMES)),
            format_func=lambda i: FLOWER_THEMES[i][0],
            index=st.session_state.settings["theme_idx"]
        )
        st.session_state.settings["theme_idx"] = theme_idx
        
        dark_mode = st.checkbox("üåô Dark Mode", value=st.session_state.settings["dark_mode"])
        st.session_state.settings["dark_mode"] = dark_mode
        
        # API Keys
        st.subheader(T["api_keys"])
        
        env_gemini = os.getenv("GOOGLE_API_KEY")
        env_openai = os.getenv("OPENAI_API_KEY")
        env_grok = os.getenv("XAI_API_KEY")
        
        gemini_key = st.text_input(
            T["gemini_key"],
            type="password",
            value="" if not env_gemini else "‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢",
            disabled=bool(env_gemini)
        )
        
        openai_key = st.text_input(
            T["openai_key"],
            type="password",
            value="" if not env_openai else "‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢",
            disabled=bool(env_openai)
        )
        
        grok_key = st.text_input(
            T["grok_key"],
            type="password",
            value="" if not env_grok else "‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢",
            disabled=bool(env_grok)
        )
        
        if st.button(T["apply_keys"], use_container_width=True):
            st.session_state.api_keys["gemini"] = gemini_key or env_gemini
            st.session_state.api_keys["openai"] = openai_key or env_openai
            st.session_state.api_keys["grok"] = grok_key or env_grok
            st.success(T["saved"])
        else:
            st.session_state.api_keys["gemini"] = st.session_state.api_keys["gemini"] or env_gemini
            st.session_state.api_keys["openai"] = st.session_state.api_keys["openai"] or env_openai
            st.session_state.api_keys["grok"] = st.session_state.api_keys["grok"] or env_grok
        
        # API Status
        st.markdown("---")
        st.markdown("**API Status:**")
        for name, key in [
            ("Gemini", st.session_state.api_keys["gemini"]),
            ("OpenAI", st.session_state.api_keys["openai"]),
            ("Grok", st.session_state.api_keys["grok"])
        ]:
            status = "‚úì" if key else "‚úó"
            color = "#4caf50" if key else "#f44336"
            st.markdown(f"<span style='color: {color}'>{status} {name}</span>", unsafe_allow_html=True)
    
    # Apply theme
    apply_theme(theme_idx, dark_mode)
    
    # Main header
    theme_name = FLOWER_THEMES[theme_idx][0]
    render_header(T, theme_name)
    
    # Main tabs
    tabs = st.tabs([
        T["docs"],
        T["ocr"],
        T["combine"],
        T["wordgraph"],
        T["agents"],
        T["dashboard"]
    ])
    
    # Tab 1: Documents
    with tabs[0]:
        render_documents_tab(T)
    
    # Tab 2: OCR
    with tabs[1]:
        render_ocr_tab(T)
    
    # Tab 3: Combine & Analyze
    with tabs[2]:
        render_combine_tab(T)
    
    # Tab 4: Word Graph Analysis
    with tabs[3]:
        render_wordgraph_tab(T)
    
    # Tab 5: Agents
    with tabs[4]:
        render_agents_tab(T)
    
    # Tab 6: Dashboard
    with tabs[5]:
        render_dashboard_tab(T)

# =============================================================================
# TAB RENDERERS
# =============================================================================
def render_agents_tab(T: dict):
    """Render agent workflows tab"""
    st.subheader(T["agents"])
    
    # Load or create agents configuration
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Upload agents.yaml
        uploaded_yaml = st.file_uploader(
            "üì§ Upload agents.yaml",
            type=["yaml", "yml"],
            key="agents_yaml_upload"
        )
        
        if uploaded_yaml:
            yaml_content = uploaded_yaml.read().decode("utf-8")
            st.session_state.agents_yaml = yaml_content
            try:
                agents_config = yaml.safe_load(yaml_content)
                st.session_state.agents = agents_config.get("agents", [])
                render_status("success", f"Loaded {len(st.session_state.agents)} agents")
            except Exception as e:
                render_status("error", f"YAML parse error: {str(e)}")
    
    with col2:
        # Download agents.yaml
        if st.session_state.agents_yaml:
            st.download_button(
                "üì• Download agents.yaml",
                data=st.session_state.agents_yaml,
                file_name=f"agents_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml",
                mime="text/yaml",
                use_container_width=True
            )
    
    # Agent YAML editor
    st.markdown("### üìù Agent Configuration Editor")
    agents_yaml_text = st.text_area(
        "Edit agents.yaml (Traditional Chinese)",
        value=st.session_state.agents_yaml,
        height=300,
        help="Define agents in YAML format. Each agent should have: name, description, system_prompt, and optional parameters."
    )
    
    if agents_yaml_text != st.session_state.agents_yaml:
        st.session_state.agents_yaml = agents_yaml_text
        try:
            agents_config = yaml.safe_load(agents_yaml_text)
            st.session_state.agents = agents_config.get("agents", [])
        except:
            pass
    
    if not st.session_state.agents:
        st.info("üìã Please upload or define agents in YAML format above")
        return
    
    st.markdown("---")
    
    # Agent selection and execution
    st.markdown("### ü§ñ Agent Execution Pipeline")
    
    # Select agents to run
    agent_names = [agent.get("name", f"Agent {i+1}") for i, agent in enumerate(st.session_state.agents)]
    selected_agent_names = st.multiselect(
        T["select_agents"],
        agent_names,
        default=agent_names[:3] if len(agent_names) >= 3 else agent_names
    )
    
    selected_agents = [agent for agent in st.session_state.agents 
                      if agent.get("name") in selected_agent_names]
    
    # Input document for agents
    st.markdown("### üìÑ Input Document")
    
    input_source = st.radio(
        "Input Source",
        ["Paste New Text", "Use Combined Document", "Previous Agent Output"],
        horizontal=True
    )
    
    if input_source == "Paste New Text":
        agent_input_doc = st.text_area(
            "Paste document content (text, markdown, json, csv)",
            height=200,
            placeholder="Paste your document here..."
        )
    elif input_source == "Use Combined Document":
        agent_input_doc = re.sub(r'<[^>]+>', '', st.session_state.combined_doc)
        st.info(f"Using combined document ({estimate_tokens(agent_input_doc)} tokens)")
    else:
        if st.session_state.agent_results:
            last_result = st.session_state.agent_results[-1]
            agent_input_doc = last_result.get("output", "")
            st.info(f"Using output from: {last_result.get('agent_name', 'Previous Agent')}")
        else:
            agent_input_doc = ""
            st.warning("No previous agent output available")
    
    # Display selected agents workflow
    if selected_agents:
        st.markdown("### üîÑ Agent Workflow")
        st.markdown("<div class='agent-workflow'>", unsafe_allow_html=True)
        
        for idx, agent in enumerate(selected_agents):
            agent_name = agent.get("name", f"Agent {idx+1}")
            agent_desc = agent.get("description", "No description")
            
            st.markdown(f"""
                <div class='agent-step'>
                    <div style='font-weight: 600; font-size: 1.1rem; margin-bottom: 0.5rem;'>
                        {idx+1}. {agent_name}
                    </div>
                    <div style='opacity: 0.8; font-size: 0.9rem;'>
                        {agent_desc}
                    </div>
                </div>
            """, unsafe_allow_html=True)
        
        st.markdown("</div>", unsafe_allow_html=True)
    
    # Execution controls
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        execution_mode = st.radio(
            "Execution Mode",
            ["Sequential (One-by-One)", "Batch (All at Once)"],
            help="Sequential: Execute agents one by one with manual review. Batch: Execute all selected agents automatically."
        )
    
    with col2:
        auto_chain = st.checkbox(
            "Auto-chain outputs",
            value=True,
            help="Use each agent's output as input for the next agent"
        )
    
    with col3:
        clear_results = st.button("üóëÔ∏è Clear Results", use_container_width=True)
        if clear_results:
            st.session_state.agent_results = []
            st.rerun()
    
    # Execute agents
    if execution_mode == "Sequential (One-by-One)":
        render_sequential_execution(selected_agents, agent_input_doc, auto_chain, T)
    else:
        render_batch_execution(selected_agents, agent_input_doc, auto_chain, T)
    
    # Display results
    if st.session_state.agent_results:
        st.markdown("---")
        st.markdown("### üìä Agent Results")
        
        for idx, result in enumerate(st.session_state.agent_results):
            with st.expander(f"‚úì {result['agent_name']} - {result['timestamp'][:19]}", expanded=idx == len(st.session_state.agent_results) - 1):
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.markdown(f"**Model:** {result['model']}")
                    st.markdown(f"**Execution Time:** {result['execution_time']:.2f}s")
                    st.markdown(f"**Tokens:** ~{result['tokens']}")
                
                with col2:
                    if st.button("üìã Copy", key=f"copy_{idx}"):
                        st.code(result['output'], language="markdown")
                
                # Editable output
                edited_output = st.text_area(
                    "Output (editable - will be used as input for next agent if auto-chain is enabled)",
                    value=result['output'],
                    height=300,
                    key=f"output_{idx}"
                )
                result['output'] = edited_output
                
                # Show follow-up questions if available
                if result.get('follow_up_questions'):
                    st.markdown("**üí° Follow-up Questions:**")
                    for q in result['follow_up_questions']:
                        st.markdown(f"- {q}")
        
        # Export all results
        if st.button("üì• Export All Results", use_container_width=True):
            export_data = {
                "workflow": [agent.get("name") for agent in selected_agents],
                "results": st.session_state.agent_results,
                "timestamp": datetime.now().isoformat()
            }
            st.download_button(
                "Download JSON",
                data=json.dumps(export_data, ensure_ascii=False, indent=2),
                file_name=f"agent_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )


def render_sequential_execution(selected_agents: List[Dict], input_doc: str, auto_chain: bool, T: dict):
    """Render sequential agent execution interface"""
    
    # Track current agent index
    if "current_agent_idx" not in st.session_state:
        st.session_state.current_agent_idx = 0
    
    if not selected_agents:
        return
    
    current_idx = st.session_state.current_agent_idx
    
    if current_idx >= len(selected_agents):
        st.success("‚úÖ All agents completed!")
        if st.button("üîÑ Restart Workflow"):
            st.session_state.current_agent_idx = 0
            st.session_state.agent_results = []
            st.rerun()
        return
    
    current_agent = selected_agents[current_idx]
    agent_name = current_agent.get("name", f"Agent {current_idx+1}")
    
    st.markdown(f"### üéØ Current Agent: {agent_name} ({current_idx + 1}/{len(selected_agents)})")
    
    with st.expander("Agent Configuration", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            # Model selection
            provider_model = st.selectbox(
                "Select Model",
                [
                    "gemini:gemini-2.5-flash",
                    "gemini:gemini-2.5-flash-lite",
                    "gemini:gemini-2.5-pro",
                    "openai:gpt-4o-mini",
                    "openai:gpt-4.1-mini",
                    "openai:gpt-5-nano",
                    "grok:grok-4-fast-reasoning",
                    "grok:grok-3-mini"
                ],
                index=0,
                key=f"model_{current_idx}"
            )
        
        with col2:
            temperature = st.slider(
                "Temperature",
                0.0, 1.0, 
                float(current_agent.get("temperature", 0.2)),
                0.1,
                key=f"temp_{current_idx}"
            )
        
        # System prompt
        system_prompt = st.text_area(
            "System Prompt",
            value=current_agent.get("system_prompt", ADVANCED_PROMPTS["agent_system"]),
            height=150,
            key=f"sys_{current_idx}"
        )
        
        # User prompt template
        user_prompt_template = st.text_area(
            "User Prompt Template (use {input} placeholder)",
            value=current_agent.get("user_prompt", "ÂàÜÊûê‰ª•‰∏ãÊñá‰ª∂Ôºö\n\n{input}"),
            height=100,
            key=f"user_{current_idx}"
        )
        
        max_tokens = st.number_input(
            "Max Tokens",
            100, 8000,
            int(current_agent.get("max_tokens", 2000)),
            key=f"tokens_{current_idx}"
        )
    
    # Determine input for current agent
    if auto_chain and st.session_state.agent_results:
        current_input = st.session_state.agent_results[-1]['output']
        st.info(f"üì• Input from previous agent: {st.session_state.agent_results[-1]['agent_name']}")
    else:
        current_input = input_doc
    
    # Preview input
    with st.expander("Preview Input Document", expanded=False):
        st.text_area("Input", value=current_input, height=200, disabled=True)
    
    # Execute button
    if st.button(f"‚ñ∂Ô∏è Execute {agent_name}", type="primary", use_container_width=True):
        if not current_input.strip():
            render_status("error", "Input document is empty")
            return
        
        with st.status(f"üîÑ Executing {agent_name}...", expanded=True) as status:
            start_time = time.time()
            
            router = LLMRouter(
                google_key=st.session_state.api_keys["gemini"],
                openai_key=st.session_state.api_keys["openai"],
                grok_key=st.session_state.api_keys["grok"]
            )
            
            provider, model = provider_model.split(":")
            user_prompt = user_prompt_template.replace("{input}", current_input)
            
            st.write(f"Provider: {provider}, Model: {model}")
            st.write(f"Input length: {len(current_input)} chars (~{estimate_tokens(current_input)} tokens)")
            
            output = router.generate_text(
                provider=provider,
                model=model,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            elapsed = time.time() - start_time
            
            # Generate follow-up questions
            follow_up_questions = generate_follow_up_questions(output, current_agent)
            
            # Save result
            result = {
                "agent_name": agent_name,
                "agent_description": current_agent.get("description", ""),
                "model": provider_model,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "input": current_input[:500] + "..." if len(current_input) > 500 else current_input,
                "output": output,
                "tokens": estimate_tokens(output),
                "execution_time": elapsed,
                "timestamp": datetime.now().isoformat(),
                "follow_up_questions": follow_up_questions
            }
            
            st.session_state.agent_results.append(result)
            st.session_state.metrics["agent_runs"] += 1
            st.session_state.metrics["total_tokens"] += estimate_tokens(output)
            st.session_state.metrics["processing_times"].append(elapsed)
            
            status.update(label=f"‚úì {agent_name} Complete", state="complete")
            render_status("success", f"Completed in {elapsed:.2f}s")
        
        # Move to next agent
        st.session_state.current_agent_idx += 1
        st.rerun()


def render_batch_execution(selected_agents: List[Dict], input_doc: str, auto_chain: bool, T: dict):
    """Render batch agent execution interface"""
    
    if st.button(f"‚ñ∂Ô∏è Execute All Agents ({len(selected_agents)})", type="primary", use_container_width=True):
        if not input_doc.strip():
            render_status("error", "Input document is empty")
            return
        
        router = LLMRouter(
            google_key=st.session_state.api_keys["gemini"],
            openai_key=st.session_state.api_keys["openai"],
            grok_key=st.session_state.api_keys["grok"]
        )
        
        current_input = input_doc
        
        with st.status(f"üîÑ Executing {len(selected_agents)} agents...", expanded=True) as status:
            for idx, agent in enumerate(selected_agents):
                agent_name = agent.get("name", f"Agent {idx+1}")
                st.write(f"[{idx+1}/{len(selected_agents)}] Executing {agent_name}...")
                
                start_time = time.time()
                
                # Get model from agent config or use default
                provider_model = agent.get("model", "gemini:gemini-2.5-flash")
                provider, model = provider_model.split(":")
                
                system_prompt = agent.get("system_prompt", ADVANCED_PROMPTS["agent_system"])
                user_prompt_template = agent.get("user_prompt", "ÂàÜÊûê‰ª•‰∏ãÊñá‰ª∂Ôºö\n\n{input}")
                user_prompt = user_prompt_template.replace("{input}", current_input)
                
                temperature = float(agent.get("temperature", 0.2))
                max_tokens = int(agent.get("max_tokens", 2000))
                
                output = router.generate_text(
                    provider=provider,
                    model=model,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                elapsed = time.time() - start_time
                
                # Generate follow-up questions
                follow_up_questions = generate_follow_up_questions(output, agent)
                
                # Save result
                result = {
                    "agent_name": agent_name,
                    "agent_description": agent.get("description", ""),
                    "model": provider_model,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "input": current_input[:500] + "..." if len(current_input) > 500 else current_input,
                    "output": output,
                    "tokens": estimate_tokens(output),
                    "execution_time": elapsed,
                    "timestamp": datetime.now().isoformat(),
                    "follow_up_questions": follow_up_questions
                }
                
                st.session_state.agent_results.append(result)
                st.session_state.metrics["agent_runs"] += 1
                st.session_state.metrics["total_tokens"] += estimate_tokens(output)
                st.session_state.metrics["processing_times"].append(elapsed)
                
                st.write(f"‚úì Completed in {elapsed:.2f}s")
                
                # Chain output to next agent if enabled
                if auto_chain:
                    current_input = output
            
            status.update(label="‚úì All Agents Complete", state="complete")
            st.balloons()


def generate_follow_up_questions(output: str, agent: Dict) -> List[str]:
    """Generate follow-up questions based on agent output"""
    questions = []
    
    agent_type = agent.get("name", "").lower()
    
    # Pattern-based question generation
    if "ÊëòË¶Å" in agent_type or "summary" in agent_type:
        questions = [
            "ÊòØÂê¶ÈúÄË¶ÅÊõ¥Ë©≥Á¥∞ÁöÑÁâπÂÆöÁ´†ÁØÄÊëòË¶ÅÔºü",
            "ÊúâÂì™‰∫õÈóúÈçµÁôºÁèæÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÂàÜÊûêÔºü",
            "ÊòØÂê¶ÈúÄË¶ÅÂ∞çÊØî‰∏çÂêåÊñá‰ª∂ÁöÑÊëòË¶ÅÔºü"
        ]
    elif "È¢®Èö™" in agent_type or "risk" in agent_type:
        questions = [
            "Âª∫Ë≠∞Êé°ÂèñÂì™‰∫õÈ¢®Èö™Á∑©Ëß£Êé™ÊñΩÔºü",
            "Â¶Ç‰ΩïÈáèÂåñÈÄô‰∫õÈ¢®Èö™ÁöÑÂΩ±ÈüøÔºü",
            "ÊòØÂê¶ÈúÄË¶ÅÂª∫Á´ãÈ¢®Èö™Áõ£ÊéßÊ©üÂà∂Ôºü"
        ]
    elif "Ê≥ïË¶è" in agent_type or "regulatory" in agent_type:
        questions = [
            "ÊòØÂê¶Á¨¶ÂêàÊúÄÊñ∞ÁöÑ FDA ÊåáÂ∞éÂéüÂâáÔºü",
            "ÈúÄË¶ÅÊ∫ñÂÇôÂì™‰∫õÈ°çÂ§ñÁöÑÂêàË¶èÊñá‰ª∂Ôºü",
            "Âª∫Ë≠∞ÁöÑÊ≥ïË¶èÊèê‰∫§ÊôÇÈñìË°®ÁÇ∫‰ΩïÔºü"
        ]
    elif "Ëó•Áâ©" in agent_type or "drug" in agent_type:
        questions = [
            "ÊòØÂê¶ÈúÄË¶ÅÊõ¥Êñ∞Ëó•Áâ©Ê®ôÁ±§‰ø°ÊÅØÔºü",
            "Âª∫Ë≠∞ÈÄ≤Ë°åÂì™‰∫õÈ°çÂ§ñÁöÑËá®Â∫äÁ†îÁ©∂Ôºü",
            "Â¶Ç‰ΩïÂÑ™ÂåñÁµ¶Ëó•ÊñπÊ°àÔºü"
        ]
    elif "‰∏çËâØ" in agent_type or "adverse" in agent_type:
        questions = [
            "ÈÄô‰∫õ‰∏çËâØ‰∫ã‰ª∂ÁöÑÂö¥ÈáçÁ®ãÂ∫¶Â¶Ç‰ΩïÔºü",
            "ÊòØÂê¶ÈúÄË¶ÅÂêë FDA Êèê‰∫§ÂÆâÂÖ®Â†±ÂëäÔºü",
            "Âª∫Ë≠∞Êé°ÂèñÂì™‰∫õÊÇ£ËÄÖÁõ£Ê∏¨Êé™ÊñΩÔºü"
        ]
    else:
        questions = [
            "ÊòØÂê¶ÈúÄË¶ÅÂ∞çÊ≠§ÂàÜÊûêÈÄ≤Ë°åÊ∑±ÂÖ•Êé¢Ë®éÔºü",
            "ÊúâÂì™‰∫õÁõ∏ÈóúÁöÑÂæåÁ∫åÁ†îÁ©∂ÊñπÂêëÔºü",
            "Â¶Ç‰ΩïÂ∞áÈÄô‰∫õÁôºÁèæÊáâÁî®Âà∞ÂØ¶ÈöõÊìç‰Ωú‰∏≠Ôºü"
        ]
    
    return questions[:3]


def render_dashboard_tab(T: dict):
    """Render analytics dashboard tab"""
    st.subheader(T["dashboard"])
    
    metrics = st.session_state.metrics
    
    # Key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        render_metric_card(T["docs_processed"], metrics["docs_processed"], "üìÑ")
    
    with col2:
        render_metric_card(T["pages_ocr"], metrics["pages_ocr"], "üîç")
    
    with col3:
        render_metric_card(T["tokens"], f"{metrics['total_tokens']:,}", "üî§")
    
    with col4:
        render_metric_card(T["agent_runs"], metrics["agent_runs"], "ü§ñ")
    
    # Processing times chart
    if metrics["processing_times"]:
        st.markdown("---")
        st.markdown("### ‚è±Ô∏è " + T["processing_time"])
        
        times_df = pd.DataFrame({
            "Run": range(1, len(metrics["processing_times"]) + 1),
            "Time (s)": metrics["processing_times"]
        })
        
        fig = px.line(
            times_df,
            x="Run",
            y="Time (s)",
            markers=True,
            title="Processing Time Trend"
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Agent performance
    if st.session_state.agent_results:
        st.markdown("---")
        st.markdown("### ü§ñ Agent Performance")
        
        agent_stats = []
        for result in st.session_state.agent_results:
            agent_stats.append({
                "Agent": result["agent_name"],
                "Model": result["model"],
                "Execution Time": result["execution_time"],
                "Tokens": result["tokens"]
            })
        
        stats_df = pd.DataFrame(agent_stats)
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.bar(
                stats_df,
                x="Agent",
                y="Execution Time",
                color="Model",
                title="Agent Execution Times"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.bar(
                stats_df,
                x="Agent",
                y="Tokens",
                color="Model",
                title="Token Usage by Agent"
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # Document statistics
    if st.session_state.docs:
        st.markdown("---")
        st.markdown("### üìä Document Statistics")
        
        doc_types = Counter([doc["type"] for doc in st.session_state.docs])
        doc_df = pd.DataFrame([
            {"Type": k.upper(), "Count": v}
            for k, v in doc_types.items()
        ])
        
        fig = px.pie(
            doc_df,
            values="Count",
            names="Type",
            title="Document Types Distribution"
        )
        st.plotly_chart(fig, use_container_width=True)


if __name__ == "__main__":
    main()
